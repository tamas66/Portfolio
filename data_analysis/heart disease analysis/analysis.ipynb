{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the possibility of heart disease with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('framingham.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with more than 3 missing values\n",
    "df.dropna(thresh=len(df.columns) - 3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping rows with several missing entries did not change anything on our dataframe. This means there was no row with such parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isna(), cbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns with missing values\n",
    "columns_with_na = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "# Describing the distribution of columns with missing values\n",
    "for col in columns_with_na:\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Mean: {df[col].mean()}\")\n",
    "    print(f\"Median: {df[col].median()}\")\n",
    "    print(f\"Mode: {df[col].mode()[0]}\")\n",
    "    print(f\"Standard Deviation: {df[col].std()}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df[col], kde=True, color='blue')\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    \n",
    "    # Plot boxplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(y=df[col], color='lightblue')\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a larger figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create the boxplot\n",
    "boxplot = df.boxplot(grid=False, rot=45, fontsize=10, patch_artist=True)\n",
    "\n",
    "# Customize the boxplot\n",
    "for box in boxplot.artists:\n",
    "    box.set_facecolor('lightblue')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Boxplot of DataFrame Columns', fontsize=15)\n",
    "plt.xlabel('Columns', fontsize=12)\n",
    "plt.ylabel('Values', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that none of the columns with missing values are normally distributed. Therefore, we will use the median to fill in the missing values, where applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring connections between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the connection between sex and the columns with missing values\n",
    "for col in columns_with_na:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=df, x=col, hue='male', bins=10, kde=True)\n",
    "    plt.title(f'{col} Distribution by Sex')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# including age groups into the model\n",
    "df2 = df.copy()\n",
    "df2['age_group'] = pd.cut(df['age'], bins=[20, 30, 40, 50, 60], labels=['20-30', '30-40', '40-50', '50-60'])\n",
    "for col in columns_with_na:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=df2, x='age_group', y=col, hue='male')\n",
    "    plt.title(f'{col} Distribution by Age Group and Sex')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plots, heartRate is mostly influenced by the sex of the person, while BMI, totChol, cigsPerDay and education are affected by both age and sex. Based on this and their distributions, I imputed the mean or median value of the respective category for each instance with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_heartrate_by_sex = df2.groupby('male')['heartRate'].mean()\n",
    "\n",
    "#Defining a function to fill missing values based on the age\n",
    "def impute_heartrate(row):\n",
    "    if pd.isnull(row['heartRate']):\n",
    "        # If heartrate is NaN, fill with mean heartrate of that age\n",
    "        return mean_heartrate_by_sex[row['male']]\n",
    "    else:\n",
    "        return row['heartRate']\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df2['heartRate'] = df2.apply(impute_heartrate, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same method used as above, but implenting a loop for all columns that are affected by both age and sex\n",
    "affected_columns = ['BMI', 'totChol', 'cigsPerDay']\n",
    "\n",
    "for col in affected_columns:\n",
    "    # Calculate median values for each age_group - sex combination and overall as well\n",
    "    median_col_by_age_sex = df2.groupby(['age_group', 'male'])[col].median()\n",
    "    overall_median = df2[col].median()\n",
    "    def impute_median(row, col=col):\n",
    "        if pd.isnull(row[col]):\n",
    "            group_median = median_col_by_age_sex.get((row['age_group'], row['male']))\n",
    "            if group_median is not None:  # Check if median exists\n",
    "                return group_median\n",
    "            else:\n",
    "                return overall_median  # Use overall median as fallback\n",
    "        else:\n",
    "            return row[col]\n",
    "\n",
    "    df2[col] = df2.apply(impute_median, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The exact same method was used for education, however the fallback in this case is 0.\n",
    "# The reason is that the age group 0-20 has no education, so the ordinal number 0 is used as a fallback.\n",
    "mode_col_by_age_sex = df2.groupby(['age_group', 'male'])['education'].median()\n",
    "\n",
    "def impute_mode(row):\n",
    "    if pd.isnull(row['education']):\n",
    "            group_median = median_col_by_age_sex.get((row['age_group'], row['male']))\n",
    "            if group_median is not None:\n",
    "                return group_median\n",
    "            else:\n",
    "                return 0\n",
    "    else:\n",
    "        return row['education']\n",
    "    \n",
    "df2['education'] = df2.apply(impute_mode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPMeds is a binary column, and based on the plots, it seems not to be affected by age and sex\n",
    "# Therefore, we can use KNN imputer to fill the missing values, that maps the most likely data point based on the other columns\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df2[['BPMeds']] = imputer.fit_transform(df2[['BPMeds']])\n",
    "# It takes the average of the 5 nearest neighbours, therefore I used round function to keep the binary distribution\n",
    "df2['BPMeds'] = df2['BPMeds'].round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop([\"age_group\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glucose did not show any correlation with age or sex either, but it contained a lot of missing values\n",
    "# Therefore, I included every other variable to the model to predict the missing values\n",
    "# The data is continuous, and the distribution is left skewed\n",
    "# I tried log transformation to make the data more normal, and also implemented outlier capping to avoid the effect of outliers\n",
    "# However the data was still not normal, and the distribution was not improved\n",
    "df3 = df2.copy()\n",
    "\n",
    "# Identifying outliers using IQR method\n",
    "Q1 = df2['glucose'].quantile(0.25)\n",
    "Q3 = df2['glucose'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Capping the outliers\n",
    "df3['glucose'] = np.where(df2['glucose'] < lower_bound, lower_bound, df2['glucose'])\n",
    "df3['glucose'] = np.where(df2['glucose'] > upper_bound, upper_bound, df2['glucose'])\n",
    "\n",
    "# Log transformation\n",
    "df3['target_log'] = np.log(df3['glucose'] + 1)\n",
    "\n",
    "print(f\"Column: target_log\")\n",
    "print(f\"Mean: {df3['target_log'].mean()}\")\n",
    "print(f\"Median: {df3['target_log'].median()}\")\n",
    "print(f\"Mode: {df3['target_log'].mode()[0]}\")\n",
    "print(f\"Standard Deviation: {df3['target_log'].std()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Plotting histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df3['target_log'], kde=True, color='blue')\n",
    "plt.title(f'Histogram of target_log')\n",
    "\n",
    "# Plotting boxplot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df3['target_log'], color='lightblue')\n",
    "plt.title(f'Boxplot of target_log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the distribution and the number of outliers, I could not use linear regression to predict the missing values\n",
    "# Therefore, I used Random Forest Regressor, which is more robust to outliers\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "df_missing = df2[df2['glucose'].isnull()]\n",
    "df_not_missing = df2[df2['glucose'].notnull()]\n",
    "\n",
    "# X_train contains the predictor variables for the rows without missing values\n",
    "X_train_imputation = df_not_missing.drop(['glucose'], axis=1)\n",
    "y_train_imputation = df_not_missing['glucose']\n",
    "\n",
    "# X_test contains the predictor variables for the rows with missing values\n",
    "X_test_imputation = df_missing.drop(['glucose'], axis=1)\n",
    "\n",
    "# training the model\n",
    "# Define and fit the Random Forest Regressor\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=5)\n",
    "model.fit(X_train_imputation, y_train_imputation)\n",
    "\n",
    "# prediction\n",
    "predicted_values = model.predict(X_test_imputation)\n",
    "\n",
    "# imputation\n",
    "df2.loc[df['glucose'].isnull(), 'glucose'] = predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the dataset for the logistic regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df2.drop([\"TenYearCHD\"], axis=1)\n",
    "target = df2[\"TenYearCHD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "cont_data = data.select_dtypes(include=['float64'])\n",
    "int_data = data.select_dtypes(include=['int64'])\n",
    "\n",
    "x_float = scaler.fit_transform(cont_data)\n",
    "x = np.concatenate((x_float, int_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, target, test_size=0.2, random_state=1312)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy: .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class imbalance heavily affected the recall rate for class 1 therefore the model is not effectively distinguishes whether someone has a chance of heart disease in the close future or not. To fix this, I try different sampling methods to fine-tune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling the majority class first using RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=4)\n",
    "x_us, y_us = rus.fit_resample(x_train, y_train)\n",
    "y_pred_us = lr.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_us))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ensemble method on the undersampled data by the ratio of 7:1 to even out the training process\n",
    "ensemble_model = EasyEnsembleClassifier(n_estimators=123, random_state=4)\n",
    "ensemble_model.fit(x_us, y_us)\n",
    "y_pred_ensemble = ensemble_model.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling the minority class by using SMOTE on the oversampled data\n",
    "smote = SMOTE(random_state=4)\n",
    "x_os, y_os = smote.fit_resample(x_us, y_us)\n",
    "lr.fit(x_os, y_os)\n",
    "y_pred_os = lr.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_os))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using class_weight='balanced' parameter in Logistic Regression on the resampled data\n",
    "# This modifies the loss function to penalize the minority class more\n",
    "lr_bal = LogisticRegression(class_weight='balanced')\n",
    "lr_bal.fit(x_os, y_os)\n",
    "y_pred_bal = lr_bal.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting probabilities and adjusting the threshold\n",
    "y_prob = lr_bal.predict_proba(x_test)[:, 1]\n",
    "\n",
    "threshold = 0.35\n",
    "y_pred_adjusted = (y_prob >= threshold).astype(int)\n",
    "print(classification_report(y_test, y_pred_adjusted))\n",
    "\n",
    "# Plotting the Precision-Recall curve to find the optimal threshold for our problem\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "plt.plot(thresholds, precision[:-1], label=\"Precision\")\n",
    "plt.plot(thresholds, recall[:-1], label=\"Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying another model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light Gradient Boosting Classifier is less sensitive to class imbalance therefore we might get a better result by using it as a classification model with the unbalance property set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = lgb.LGBMClassifier(is_unbalance=True, random_state=42)\n",
    "lg.fit(x_train, y_train)\n",
    "y_pred_lg = lg.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_lg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite applying various sampling methods and balancing algorithms, class imbalance remained a significant issue in the heart disease prediction model. The precision of the positive class consistently failed to exceed 0.4, which, as shown in the precision-recall curve, represents the best balance point between these two metrics.\n",
    "\n",
    "Given that our primary objective is to identify true positive cases of heart disease with the highest accuracy, maximizing recall for class 1 is paramount. This ensures that we catch as many actual cases of heart disease as possible, even if it means sacrificing some precision or overall accuracy. Therefore I have set the threshold based on the plot, to 0.35 to keep a reasonable amount of precision (and F1-Score) while having a good recall value. Missing true positive cases can have serious implications in this context, so prioritizing recall is critical, despite the presence of imbalanced data.\n",
    "\n",
    "In a final attempt to address this challenge, I implemented a different classifier to compare its ability to manage imbalance. Unfortunately, the results were similar to those observed with logistic regression, indicating that the fundamental issue persists across different models. This suggests that more advanced techniques—such as cost-sensitive learning or further tuning of the decision threshold—may be required to enhance the model's performance in predicting heart disease."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
